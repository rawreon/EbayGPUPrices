# Ebay prices of Sold Reference GPUs
### Data Collected on 10/19/2021

<p align="center">
  <img width="700" height="432" src=/images/gpuprices.png>
</p>

Using selenium and and BeautifulSoup in python, I scraped the prices of sold reference GPUs off ebay into excel spreadsheets. I cleaned up the data in python and then visualized it using ggplot in R to make the graph above. **The graph shows that reference cards are being sold at well above their MSRP, with people willing to pay a hefty premium to snag one of these reference GPUs.** 

This project was inspired by [this](https://www.reddit.com/r/dataisbeautiful/comments/q9y3ne/oc_sale_prices_of_used_iphones/) post on /r/dataisbeautiful that I saw on the front page of reddit. I discovered that this was a sina plot and wanted to create my own graphic with this type of graph. The reddit post used the listing prices of iphones on ebay, but I thought the price of completed sold items were more useful so I wanted decided to head in that direction. I knew that there is a shortage for graphics cards as the chip shortage and increase in cryptocurrency mining has led cards being sold at a premium, and so I want to see how much above MSRP graphics cards are being sold on Ebay. Reference graphic cards are GPUs manufactured by NVIDIA and AMD themselves. I decided to only look at reference cards for the most recent GPU generation as looking at every brand that makes multiple GPUs would have taken a lot longer. A few comments speculated about how the data was collected as the poster did not specify. The two main sources was using Ebay's API and webscraping. I decided to try using Ebay API's first.

## Using Ebay API and its Limits
After creating an ebay developer account and applying for production API keys, I followed [this](https://www.youtube.com/watch?v=Ma_eLdobmlM) wonderful tutorial by Python 360. Using [ebaySDK](https://github.com/timotheus/ebaysdk-python) to make requests, I was able to sucessfully retrieve listings of GPUs. By specifying aspect filters and item filters in the parameters of my request, I retrieved the listings of only reference GPUs. However, this is when I ran into a bit of a problem. In turns out that Ebay's API was only able to return current listings and not completed sold listings. There is a parameter, SoldItemsOnly, that I could specify for item filter, but it was only a placeholder that was not functional. You used to be able to specify findCompletedItems in the request, but this call is depreciated and no longer useable. There exists a marketplace API, but in order to gain access I had to apply for business approval. At this point, I either had to use the listing prices like the reddit post, or find another avenue for collecting data. This led me to try webscraping in python.

## Obtaining Data through Webscraping using Selenium and BeautifulSoup
Webscraping was not as complicated as I thought it would be and was actually kind of fun. It was mostly finding the right classes and ids to reference in a website's HTML. Learning to automate actions on the web through the use of webdrivers was really cool to me. [This](https://www.youtube.com/watch?v=9ELd7XWD0PA) amazing tutorial by Jie Jenn provided the framework for my data collection of completed sold listings. There were a few roadblocks along the way of collecting my data with webscraping. The reference class for current listings is different for completed listings. It turned out the HTML class reference for completed listings also referenced an empty space before the listings started, and so when the code tried to look for data within that empty listing, it would stop functioning and return nothing. It took a frustrating hour to figure this out. The easy fix for this was to just pop the first item of my listing array, thereby removing the empty listing and working with actual listings. Another problem, and the most annoying problem, I encountered was dealing with CAPTCHA. When looking at sold completed listings, they would make the user finish a CAPTCHA before letting them access the page. After much research, I found no ways to circumvent the CAPTCHA. My solution to this problem was to set a timer after opening the specified URL in my code, before the webscraping starts. If there is a CAPTCHA, this timer allowed me to manually solve it, letting the webscraping function to collect data like normal. I ran this function ten times, obtaining an excel file for each reference GPU I looked at that contained the listing name, the date the listing ended, and the price the GPU sold for. 

## Data Cleaning and Visualization
For each excel sheet, I added the name of the GPU as a column as there was no real reference to which GPU each listing was for. In hindsight, I could have specified this when creating the excel sheets when webscraping, saving me some work. I combined all excel sheets into a single dataframe in python, and then worked cleaning the dataframe for creating my visualization. The only three columns that mattered was the name of the GPU, date sold, and sold price. After obtaining a dataframe with only these three columns, I worked on setting the appropriate types for each column. Each column was initially type object, so I changed GPU name into type string, date sold into type datetime, and sold price to type float. Since sold price contained dollar signs and commas in the prices, I stripped these characters before converting into floats. 

After obtaining the cleaned dataframe, it was time to create my graphs. I exported the dataframe as a csv file and then loaded it into R studio. After installing the library ggforce that contained the geom_sina function for ggplot, I tried to create a similar graph to the reddit post with the names of GPUs on the Y axis and their sold prices on the X axis. I wanted to highlight the MSRP for each GPU on my graph by adding a solid bar that represented the MSRP, so I created an excel file containing each GPU and their MSRPs. I imported this data into R studio and then added a crossbar and a linerange to my ggplot. I had to do this because this was the only way that I could get a bar on my graph, as well as a legend. The crossbar corresponded to the bar shown on my graph, while the linerange is what populated the legend. For the purpose of my visual, they look exactly the same and without one, either the bar on the graph or the legend would be missing. After adding some color and cleaning up the GPU names, I obtained my final graph.

I also graphed a scatterplot of the relationship between date on the X axis and price on the Y axis, which is shown below. I did not bother cleaning this graph up as the results were not as interesting. I thought that *maybe* the premium for GPUs would go down as time went by, but they have stayed relatively the same, even with some showing a trend of the price increasing. 

<p align="center">
  <img width="700" height="432" src=/images/dateprice.png>
</p>

## A Review and Future Considerations
**It is important to note that Ebay only saves data for sold completed listings for the past three months. This means the data I collected on 10/19/2021 would no longer be considered if someone else was to collect data on a later date.**

I did not do much in terms of cleaning outliers in my data as they did not take away from what I wanted to show in my graph. A lot of the lower outliers that were below MSRP were either selling parts for reference cards, or charged most of the price through shipping. In hindsight I could have added shipping cost as well, therefore fixing a lot of the lower outliers. 

Future projects could include not just looking at reference cards, but branded cards as well. This would take a **LOT** longer as there are dozens of brands that produce graphics cards, with some brands even producing multiple types of the same GPU. 

Overall I really enjoyed this project. I learned a lot about using ebaySDK to make requests, and even more about webscraping. The sky's the limit for automating web tasks with selenium, which may lead me to a future project. Hope this write-up gave at least a window view into the process that went into completing this project!